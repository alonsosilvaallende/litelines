{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Litelines: Structured LLM Generation","text":""},{"location":"#what-is-litelines","title":"What is Litelines?","text":"<p>Litelines is a lightweight Python library designed for structured generation with Large Language Models (LLMs), featuring powerful visualization capabilities to inspect and understand state transitions during the generation process.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Structured Generation: Control and guide LLM outputs with predefined structures and constraints</li> <li>State Transition Visualization: Interactive visual tools to monitor and analyze how your LLM progresses through different generation states</li> <li>Lightweight Design: Minimal dependencies and efficient performance for seamless integration</li> <li>Developer-Friendly: Intuitive API designed for both research and production environments</li> <li>Debugging &amp; Inspection: Deep insights into LLM behavior through comprehensive state tracking</li> </ul>"},{"location":"#why-choose-litelines","title":"Why Choose Litelines?","text":"<p>Litelines bridges the gap between raw LLM capabilities and structured, predictable outputs. Whether you're building conversational AI, content generation systems, or research tools, Litelines provides the transparency and control needed to understand and optimize your LLM's decision-making process.</p> <p>Perfect for developers who need to:</p> <ul> <li>Debug complex LLM generation workflows</li> <li>Ensure consistent, structured outputs</li> <li>Visualize and analyze model behavior</li> <li>Build reliable LLM-powered applications</li> </ul>"},{"location":"getting-started/","title":"Getting started","text":"<p>This guide will walk you through the basics of using Litelines to get structured generation from language models. By the end, you'll understand how to:</p> <ol> <li>Install Litelines</li> <li>Generate a basic structured response</li> <li>Generate a basic streamed structured response</li> </ol>"},{"location":"getting-started/#installation","title":"Installation","text":"<p>To install Litelines:</p> pipuv <pre><code>pip install litelines\n</code></pre> <pre><code>uv pip install litelines\n</code></pre>"},{"location":"getting-started/#your-first-structured-generation","title":"Your First Structured Generation","text":"<p>Let's start with a simple example.</p>"},{"location":"getting-started/#download-a-model-and-its-tokenizer","title":"Download a model and its tokenizer","text":"transformersvllm <pre><code>import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ndevice = torch.device(\"cuda\") # \"cuda\", \"mps\", or \"cpu\"\n\nmodel_id = \"Qwen/Qwen3-0.6B\"\nmodel = AutoModelForCausalLM.from_pretrained(model_id).to(device)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n</code></pre>"},{"location":"getting-started/#prepare-the-inputs-to-the-llm","title":"Prepare the inputs to the LLM","text":"transformersvllm <pre><code>user_input = \"What is the sentiment of the following text: 'Awesome'\"\nmessages = [{\"role\": \"user\", \"content\": user_input}]\ninputs = tokenizer.apply_chat_template(\n    messages, \n    add_generation_prompt=True, \n    return_tensors=\"pt\", \n    return_dict=True\n).to(model.device)\n</code></pre>"},{"location":"getting-started/#define-a-pydantic-schema-describing-the-required-json","title":"Define a Pydantic schema describing the required JSON","text":"transformersvllm <pre><code>from typing import Literal\nfrom pydantic import BaseModel, Field\n\nclass Sentiment(BaseModel):\n    \"\"\"Correctly inferred `Sentiment`.\"\"\"\n    label: Literal[\"positive\", \"negative\"] = Field(\n        ..., description=\"Sentiment of the text\"\n    )\n</code></pre>"},{"location":"getting-started/#define-the-processor-and-visualize-it","title":"Define the processor and visualize it","text":"transformersvllm <pre><code>from litelines.transformers import SchemaProcessor\n\nprocessor = SchemaProcessor(response_format=Sentiment, tokenizer=tokenizer)\nprocessor.show_graph()\n</code></pre> <p>%3Allowed PathsRegular expression: [\\n\\t ]*{[\\n\\t ]*\"label\"[\\n\\t ]*:[\\n\\t ]*(\"positive\"|\"negative\")[\\n\\t ]*}000-&gt;0idtoken2345959101503......110-&gt;1idtoken53632{1476{90{......23230-&gt;23idtoken4913{\"5212{\"1-&gt;1idtoken2345959101503......221-&gt;2idtoken92667\"label1-&gt;23idtoken330\"1\"332-&gt;3idtoken15620\"1837\"698\"......442-&gt;4idtoken788\":51418\":4660\":552-&gt;5idtoken3252\":\"3-&gt;3idtoken2345959101503......3-&gt;4idtoken25:47446:549:......3-&gt;5idtoken2974:\"34638:\"4-&gt;4idtoken2345959101503......4-&gt;5idtoken330\"1\"665-&gt;6idtoken2724posit885-&gt;8idtoken30487positive11115-&gt;11idtoken77n12125-&gt;12idtoken811ne13135-&gt;13idtoken28775neg16165-&gt;16idtoken42224negative19195-&gt;19idtoken79p20205-&gt;20idtoken966pos22225-&gt;22idtoken5368po776-&gt;7idtoken72i6-&gt;8idtoken533ive10106-&gt;10idtoken344iv7-&gt;8idtoken586ve7-&gt;10idtoken85v998-&gt;9idtoken15620\"698\"1\"......28288-&gt;28idtoken9207\"}9-&gt;9idtoken2345959101503......9-&gt;28idtoken92}335}10-&gt;8idtoken68e11-&gt;12idtoken68e11-&gt;13idtoken791eg141411-&gt;14idtoken11188ega11-&gt;16idtoken15060egative12-&gt;13idtoken70g12-&gt;14idtoken6743ga13-&gt;14idtoken64a151513-&gt;15idtoken9307ati13-&gt;16idtoken1388ative171713-&gt;17idtoken19488ativ181813-&gt;18idtoken266at14-&gt;15idtoken10251ti14-&gt;18idtoken83t15-&gt;16idtoken586ve15-&gt;17idtoken85v16-&gt;9idtoken15620\"698\"1\"......16-&gt;28idtoken9207\"}17-&gt;16idtoken68e18-&gt;15idtoken72i18-&gt;16idtoken533ive18-&gt;17idtoken344iv19-&gt;6idtoken34054osit19-&gt;20idtoken436os212119-&gt;21idtoken30724osi19-&gt;22idtoken78o20-&gt;6idtoken275it20-&gt;7idtoken12303iti20-&gt;8idtoken3404itive20-&gt;21idtoken72i21-&gt;6idtoken83t21-&gt;7idtoken10251ti22-&gt;6idtoken46865sit22-&gt;20idtoken82s22-&gt;21idtoken6321si23-&gt;2idtoken1502label242423-&gt;24idtoken14380lab262623-&gt;26idtoken75l272723-&gt;27idtoken4260la24-&gt;2idtoken301el252524-&gt;25idtoken68e25-&gt;2idtoken75l26-&gt;2idtoken780abel26-&gt;24idtoken370ab26-&gt;25idtoken8229abe26-&gt;27idtoken64a27-&gt;2idtoken9779bel27-&gt;24idtoken65b27-&gt;25idtoken1371be-&gt;0</p>"},{"location":"getting-started/#generate-a-structured-response","title":"Generate a structured response","text":"transformersvllm <pre><code>generated = model.generate(**inputs, logits_processor=[processor])\nprint(tokenizer.decode(generated[0][inputs['input_ids'].shape[-1]:]))\n# {\"label\": \"positive\"}\n</code></pre>"},{"location":"getting-started/#visualize-the-selected-path","title":"Visualize the selected path","text":"transformersvllm <pre><code>processor.show_graph()\n</code></pre> <p>%3Allowed PathsRegular expression: [\\n\\t ]*{[\\n\\t ]*\"label\"[\\n\\t ]*:[\\n\\t ]*(\"positive\"|\"negative\")[\\n\\t ]*}000-&gt;0idtoken2345959101503......110-&gt;1idtoken53632{1476{90{......23230-&gt;23idtoken4913{\"5212{\"1-&gt;1idtoken2345959101503......221-&gt;2idtoken92667\"label1-&gt;23idtoken330\"1\"332-&gt;3idtoken15620\"1837\"698\"......442-&gt;4idtoken788\":51418\":4660\":552-&gt;5idtoken3252\":\"3-&gt;3idtoken2345959101503......3-&gt;4idtoken25:47446:549:......3-&gt;5idtoken2974:\"34638:\"4-&gt;4idtoken2345959101503......4-&gt;5idtoken330\"1\"665-&gt;6idtoken2724posit885-&gt;8idtoken30487positive11115-&gt;11idtoken77n12125-&gt;12idtoken811ne13135-&gt;13idtoken28775neg16165-&gt;16idtoken42224negative19195-&gt;19idtoken79p20205-&gt;20idtoken966pos22225-&gt;22idtoken5368po776-&gt;7idtoken72i6-&gt;8idtoken533ive10106-&gt;10idtoken344iv7-&gt;8idtoken586ve7-&gt;10idtoken85v998-&gt;9idtoken15620\"698\"1\"......28288-&gt;28idtoken9207\"}9-&gt;9idtoken2345959101503......9-&gt;28idtoken92}335}10-&gt;8idtoken68e11-&gt;12idtoken68e11-&gt;13idtoken791eg141411-&gt;14idtoken11188ega11-&gt;16idtoken15060egative12-&gt;13idtoken70g12-&gt;14idtoken6743ga13-&gt;14idtoken64a151513-&gt;15idtoken9307ati13-&gt;16idtoken1388ative171713-&gt;17idtoken19488ativ181813-&gt;18idtoken266at14-&gt;15idtoken10251ti14-&gt;18idtoken83t15-&gt;16idtoken586ve15-&gt;17idtoken85v16-&gt;9idtoken15620\"698\"1\"......16-&gt;28idtoken9207\"}17-&gt;16idtoken68e18-&gt;15idtoken72i18-&gt;16idtoken533ive18-&gt;17idtoken344iv19-&gt;6idtoken34054osit19-&gt;20idtoken436os212119-&gt;21idtoken30724osi19-&gt;22idtoken78o20-&gt;6idtoken275it20-&gt;7idtoken12303iti20-&gt;8idtoken3404itive20-&gt;21idtoken72i21-&gt;6idtoken83t21-&gt;7idtoken10251ti22-&gt;6idtoken46865sit22-&gt;20idtoken82s22-&gt;21idtoken6321si23-&gt;2idtoken1502label242423-&gt;24idtoken14380lab262623-&gt;26idtoken75l272723-&gt;27idtoken4260la24-&gt;2idtoken301el252524-&gt;25idtoken68e25-&gt;2idtoken75l26-&gt;2idtoken780abel26-&gt;24idtoken370ab26-&gt;25idtoken8229abe26-&gt;27idtoken64a27-&gt;2idtoken9779bel27-&gt;24idtoken65b27-&gt;25idtoken1371be-&gt;0</p>"},{"location":"getting-started/#your-first-streamed-structured-generation","title":"Your First Streamed Structured Generation","text":"<p>Since Litelines gives you the processor, you can do whatever you want with it. In particular, you can generate a streaming response like you would normally do (just don't forget to add the processor).</p> transformersvllm <pre><code>from threading import Thread\nfrom transformers import TextIteratorStreamer\n\nstreamer = TextIteratorStreamer(tokenizer, skip_prompt=True)\ngeneration_kwargs = dict(\n    inputs, streamer=streamer, logits_processor=[processor], max_new_tokens=100\n)\n\nthread = Thread(target=model.generate, kwargs=generation_kwargs)\nthread.start()\n\nassistant_response = \"\"\nfor chunk in streamer:\n    if tokenizer.eos_token in chunk or tokenizer.pad_token in chunk:\n        chunk = chunk.split(tokenizer.eos_token)[0]\n        chunk = chunk.split(tokenizer.pad_token)[0]\n    assistant_response += chunk\n    print(chunk, end=\"\")\n\nthread.join()\n</code></pre>"},{"location":"installation/","title":"Installation","text":"<p>To install <code>litelines</code>:</p> pipuv <pre><code>pip install litelines\n</code></pre> <pre><code>uv pip install litelines\n</code></pre>"},{"location":"reference/","title":"Reference","text":""},{"location":"reference/#build_regex","title":"<code>build_regex</code>","text":"<p>Convert a Pydantic model or JSON schema to a regex.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from typing import Literal\n&gt;&gt;&gt; from pydantic import BaseModel, Field\n&gt;&gt;&gt; from litelines import build_regex\n&gt;&gt;&gt;\n&gt;&gt;&gt; class Sentiment(BaseModel):\n...     \"Correctly inferred `Sentiment` with all the required parameters with correct types.\"\n...\n...     label: Literal[\"positive\", \"negative\"] = Field(\n...         ..., description=\"Sentiment of the text\"\n...     )\n&gt;&gt;&gt; build_regex(Sentiment, whitespace_pattern=\"\")\n'\\\\{\"label\":(\"positive\"|\"negative\")\\\\}'\n&gt;&gt;&gt; build_regex(Sentiment, whitespace_pattern=\"[ ]?\")\n'[ ]?\\\\{[ ]?\"label\"[ ]?:[ ]?(\"positive\"|\"negative\")[ ]?\\\\}'\n&gt;&gt;&gt; build_regex(Sentiment)\n'[\\\\n\\\\t ]*\\\\{[\\\\n\\\\t ]*\"label\"[\\\\n\\\\t ]*:[\\\\n\\\\t ]*(\"positive\"|\"negative\")[\\\\n\\\\t ]*\\\\}'\n&gt;&gt;&gt; build_regex(Sentiment, include_tool_call=True, whitespace_pattern=\"\")\n'&lt;tool_call&gt;\\\\{\"name\":\"Sentiment\",\"arguments\":\\\\{\"label\":(\"positive\"|\"negative\")\\\\}\\\\}&lt;/tool_call&gt;'\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>Union[dict, str, Type[Any]]</code> <p>The Pydantic model or JSON schema.</p> required <code>include_tool_call</code> <code>optional</code> <p>Is the Pydantic model expecting a tool call or not.</p> <code>False</code> <code>tool_call_start</code> <code>optional</code> <p>The expected tool call start.</p> <code>'&lt;tool_call&gt;'</code> <code>tool_call_end</code> <code>optional</code> <p>The expected tool call end.</p> <code>'&lt;/tool_call&gt;'</code> <code>whitespace_pattern</code> <code>optional</code> <p>Pattern to use for JSON syntactic whitespace.</p> <code>'[\\\\n\\\\t ]*'</code> <p>Returns:</p> Type Description <code>str</code> <p>The JSON schema converted to a regex.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>An error occurs if the schema is not a Pydantic model, a dictionary, or a string.</p> Source code in <code>src/litelines/build_regex.py</code> <pre><code>def build_regex(\n    schema: Union[dict, str, Type[Any]],\n    include_tool_call: bool = False,\n    tool_call_start: str = \"&lt;tool_call&gt;\",\n    tool_call_end: str = \"&lt;/tool_call&gt;\",\n    whitespace_pattern: str = r\"[\\n\\t ]*\",\n) -&gt; str:\n    \"\"\"Convert a Pydantic model or JSON schema to a regex.\n\n    Examples:\n        &gt;&gt;&gt; from typing import Literal\n        &gt;&gt;&gt; from pydantic import BaseModel, Field\n        &gt;&gt;&gt; from litelines import build_regex\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; class Sentiment(BaseModel):\n        ...     \"Correctly inferred `Sentiment` with all the required parameters with correct types.\"\n        ...\n        ...     label: Literal[\"positive\", \"negative\"] = Field(\n        ...         ..., description=\"Sentiment of the text\"\n        ...     )\n        &gt;&gt;&gt; build_regex(Sentiment, whitespace_pattern=\"\")\n        '\\\\\\\\{\"label\":(\"positive\"|\"negative\")\\\\\\\\}'\n        &gt;&gt;&gt; build_regex(Sentiment, whitespace_pattern=\"[ ]?\")\n        '[ ]?\\\\\\\\{[ ]?\"label\"[ ]?:[ ]?(\"positive\"|\"negative\")[ ]?\\\\\\\\}'\n        &gt;&gt;&gt; build_regex(Sentiment)\n        '[\\\\\\\\n\\\\\\\\t ]*\\\\\\\\{[\\\\\\\\n\\\\\\\\t ]*\"label\"[\\\\\\\\n\\\\\\\\t ]*:[\\\\\\\\n\\\\\\\\t ]*(\"positive\"|\"negative\")[\\\\\\\\n\\\\\\\\t ]*\\\\\\\\}'\n        &gt;&gt;&gt; build_regex(Sentiment, include_tool_call=True, whitespace_pattern=\"\")\n        '&lt;tool_call&gt;\\\\\\\\{\"name\":\"Sentiment\",\"arguments\":\\\\\\\\{\"label\":(\"positive\"|\"negative\")\\\\\\\\}\\\\\\\\}&lt;/tool_call&gt;'\n\n    Args:\n        schema: The Pydantic model or JSON schema.\n        include_tool_call (optional): Is the Pydantic model expecting a tool call or not.\n        tool_call_start (optional): The expected tool call start.\n        tool_call_end (optional): The expected tool call end.\n        whitespace_pattern (optional): Pattern to use for JSON syntactic whitespace.\n\n    Returns:\n        The JSON schema converted to a regex.\n\n    Raises:\n        ValueError: An error occurs if the schema is not a Pydantic model, a dictionary, or a string.\n    \"\"\"\n    if isinstance(schema, dict):\n        schema_str = json.dumps(schema)\n        name_str = schema[\"title\"]\n    elif isinstance(schema, str):\n        schema_str = schema\n        name_str = json.loads(schema)[\"title\"]\n    elif hasattr(schema, \"model_json_schema\"):\n        schema_str = json.dumps(schema.model_json_schema())\n        name_str = schema.__name__\n    else:\n        raise ValueError(\n            f\"Cannot parse schema {schema}. The schema must be either \"\n            + \"a Pydantic class, a dictionary or a string that contains the JSON \"\n            + \"schema specification\"\n        )\n    _regex_str = build_regex_from_schema(\n        schema_str, whitespace_pattern=whitespace_pattern\n    )\n    if include_tool_call:\n        regex_str = (\n            whitespace_pattern\n            + tool_call_start\n            + whitespace_pattern\n            + \"\\\\{\"\n            + whitespace_pattern\n            + '\"name\"'\n            + whitespace_pattern\n            + \":\"\n            + whitespace_pattern\n            + '\"'\n            + name_str\n            + '\"'\n            + whitespace_pattern\n            + \",\"\n            + whitespace_pattern\n            + '\"arguments\"'\n            + whitespace_pattern\n            + \":\"\n            + whitespace_pattern\n            + _regex_str\n            + whitespace_pattern\n            + \"\\\\}\"\n            + whitespace_pattern\n            + tool_call_end\n        )\n    else:\n        regex_str = whitespace_pattern + _regex_str\n    return regex_str\n</code></pre>"},{"location":"reference/#build_dfa","title":"<code>build_dfa</code>","text":"<p>Build a deterministic finite automaton that fullfils the response format requirement</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from typing import Literal\n&gt;&gt;&gt; from pydantic import BaseModel, Field\n&gt;&gt;&gt; from transformers import AutoTokenizer\n&gt;&gt;&gt; from litelines import build_dfa\n&gt;&gt;&gt;\n&gt;&gt;&gt; model_id = \"Qwen/Qwen3-0.6B\"\n&gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(model_id)\n&gt;&gt;&gt; build_dfa(\"A|B\", tokenizer)\n{0: {33: 1, 32: 1}}\n&gt;&gt;&gt; build_dfa(\"A0|B0\", tokenizer)\n{1: {15: 3}, 2: {15: 3}, 0: {33: 1, 32: 2}}\n&gt;&gt;&gt;\n&gt;&gt;&gt; class Sentiment(BaseModel):\n...     \"Correctly inferred `Sentiment` with all the required parameters with correct types.\"\n...\n...     label: Literal[\"positive\", \"negative\"] = Field(\n...         ..., description=\"Sentiment of the text\"\n...     )\n&gt;&gt;&gt; build_dfa(Sentiment, tokenizer, whitespace_pattern=\"\")\n{18: {72: 15, 344: 17, 533: 16}, 9: {92: 28}, 20: {72: 21, 12303: 7, 275: 6, 3404: 8}, 23: {2974: 5, 25: 24}, 1: {14380: 2, 75: 25, 4260: 26, 1502: 4}, 14: {10251: 15, 83: 18}, 8: {9207: 28, 1: 9}, 22: {82: 20, 6321: 21, 46865: 6}, 4: {3252: 5, 1: 23, 788: 24}, 0: {4913: 1, 90: 27}, 13: {64: 14, 19488: 17, 266: 18, 1388: 16, 9307: 15}, 10: {68: 8}, 19: {436: 20, 78: 22, 34054: 6, 30724: 21}, 3: {75: 4}, 16: {9207: 28, 1: 9}, 12: {70: 13, 6743: 14}, 7: {586: 8, 85: 10}, 11: {68: 12, 15060: 16, 11188: 14, 791: 13}, 2: {68: 3, 301: 4}, 17: {68: 16}, 27: {92667: 4, 1: 1}, 6: {72: 7, 344: 10, 533: 8}, 5: {2724: 6, 77: 11, 28775: 13, 42224: 16, 79: 19, 5368: 22, 30487: 8, 966: 20, 811: 12}, 26: {1371: 3, 65: 2, 9779: 4}, 15: {586: 16, 85: 17}, 21: {10251: 7, 83: 6}, 24: {1: 5}, 25: {370: 2, 64: 26, 780: 4, 8229: 3}}\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>response_format</code> <code>Union[dict, str, Type[Any]]</code> <p>A Pydantic model, a dictionary, or a regular expression (as a string) that defines the expected response format</p> required <code>tokenizer</code> <code>Union[str, PreTrainedTokenizer, PreTrainedTokenizerFast]</code> <p>The model's tokenizer or the model name (as a string)</p> required <code>include_tool_call</code> <code>optional</code> <p>Is the Pydantic model expecting a tool call or not.</p> <code>False</code> <code>tool_call_start</code> <code>optional</code> <p>The expected tool call start.</p> <code>'&lt;tool_call&gt;'</code> <code>tool_call_end</code> <code>optional</code> <p>The expected tool call end.</p> <code>'&lt;/tool_call&gt;'</code> <code>whitespace_pattern</code> <code>optional</code> <p>Pattern to use for JSON syntactic whitespace.</p> <code>'[\\\\n\\\\t\\\\r ]*'</code> <p>Returns:</p> Type Description <code>dict[int, dict[int, int]]</code> <p>The deterministic finite automaton as a dictionary.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>An error occurs if the response format is not a Pydantic model, a dictionary, or a string that corresponds to the regular expression.</p> Source code in <code>src/litelines/build_dfa.py</code> <pre><code>def build_dfa(\n    response_format: Union[dict, str, Type[Any]],\n    tokenizer: Union[str, PreTrainedTokenizer, PreTrainedTokenizerFast],\n    include_tool_call: bool = False,\n    tool_call_start: str = \"&lt;tool_call&gt;\",\n    tool_call_end: str = \"&lt;/tool_call&gt;\",\n    whitespace_pattern: str = r\"[\\n\\t\\r ]*\",\n) -&gt; dict[int, dict[int, int]]:\n    \"\"\"Build a deterministic finite automaton that fullfils the response format requirement\n\n    Examples:\n        &gt;&gt;&gt; from typing import Literal\n        &gt;&gt;&gt; from pydantic import BaseModel, Field\n        &gt;&gt;&gt; from transformers import AutoTokenizer\n        &gt;&gt;&gt; from litelines import build_dfa\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; model_id = \"Qwen/Qwen3-0.6B\"\n        &gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(model_id)\n        &gt;&gt;&gt; build_dfa(\"A|B\", tokenizer)\n        {0: {33: 1, 32: 1}}\n        &gt;&gt;&gt; build_dfa(\"A0|B0\", tokenizer)\n        {1: {15: 3}, 2: {15: 3}, 0: {33: 1, 32: 2}}\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; class Sentiment(BaseModel):\n        ...     \"Correctly inferred `Sentiment` with all the required parameters with correct types.\"\n        ...\n        ...     label: Literal[\"positive\", \"negative\"] = Field(\n        ...         ..., description=\"Sentiment of the text\"\n        ...     )\n        &gt;&gt;&gt; build_dfa(Sentiment, tokenizer, whitespace_pattern=\"\")\n        {18: {72: 15, 344: 17, 533: 16}, 9: {92: 28}, 20: {72: 21, 12303: 7, 275: 6, 3404: 8}, 23: {2974: 5, 25: 24}, 1: {14380: 2, 75: 25, 4260: 26, 1502: 4}, 14: {10251: 15, 83: 18}, 8: {9207: 28, 1: 9}, 22: {82: 20, 6321: 21, 46865: 6}, 4: {3252: 5, 1: 23, 788: 24}, 0: {4913: 1, 90: 27}, 13: {64: 14, 19488: 17, 266: 18, 1388: 16, 9307: 15}, 10: {68: 8}, 19: {436: 20, 78: 22, 34054: 6, 30724: 21}, 3: {75: 4}, 16: {9207: 28, 1: 9}, 12: {70: 13, 6743: 14}, 7: {586: 8, 85: 10}, 11: {68: 12, 15060: 16, 11188: 14, 791: 13}, 2: {68: 3, 301: 4}, 17: {68: 16}, 27: {92667: 4, 1: 1}, 6: {72: 7, 344: 10, 533: 8}, 5: {2724: 6, 77: 11, 28775: 13, 42224: 16, 79: 19, 5368: 22, 30487: 8, 966: 20, 811: 12}, 26: {1371: 3, 65: 2, 9779: 4}, 15: {586: 16, 85: 17}, 21: {10251: 7, 83: 6}, 24: {1: 5}, 25: {370: 2, 64: 26, 780: 4, 8229: 3}}\n\n    Args:\n        response_format: A Pydantic model, a dictionary, or a regular expression (as a string) that defines the expected response format\n        tokenizer: The model's tokenizer or the model name (as a string)\n        include_tool_call (optional): Is the Pydantic model expecting a tool call or not.\n        tool_call_start (optional): The expected tool call start.\n        tool_call_end (optional): The expected tool call end.\n        whitespace_pattern (optional): Pattern to use for JSON syntactic whitespace.\n\n    Returns:\n        The deterministic finite automaton as a dictionary.\n\n    Raises:\n        ValueError: An error occurs if the response format is not a Pydantic model, a dictionary, or a string that corresponds to the regular expression.\n    \"\"\"\n    if isinstance(response_format, str):\n        if is_valid_json(response_format):\n            regex_str = build_regex(\n                response_format,\n                include_tool_call=include_tool_call,\n                whitespace_pattern=whitespace_pattern,\n            )\n        elif is_valid_regex(response_format):\n            regex_str = response_format\n        else:\n            invalid_schema_error(response_format)\n    elif isinstance(response_format, dict) or hasattr(\n        response_format, \"model_json_schema\"\n    ):\n        regex_str = build_regex(\n            response_format,\n            include_tool_call=include_tool_call,\n            tool_call_start=tool_call_start,\n            tool_call_end=tool_call_end,\n            whitespace_pattern=whitespace_pattern,\n        )\n    else:\n        invalid_schema_error(response_format)\n\n    if isinstance(tokenizer, str):\n        model_name = tokenizer\n    elif isinstance(tokenizer, (PreTrainedTokenizer, PreTrainedTokenizerFast)):\n        model_name = getattr(tokenizer, \"name_or_path\", None)\n        if model_name is None:\n            raise ValueError(\n                \"Could not determine model name from tokenizer. \"\n                + \"You can pass it directly to the build_dfa function.\"\n            )\n    else:\n        raise ValueError(\n            \"The tokenizer must be either \"\n            + \"a PreTrainedTokenizer, a PreTrainedTokenizerFast \"\n            + \"or a string that corresponds to the model name.\"\n        )\n\n    vocabulary = Vocabulary.from_pretrained(model_name)\n    index = Index(regex_str, vocabulary)\n    dfa = get_dfa(index)\n    return dfa\n</code></pre>"},{"location":"reference/#draw_dfa","title":"<code>draw_dfa</code>","text":"<p>Create a graphical representation of a Deterministic Finite Automaton (DFA) using Graphviz DOT language.</p> <p>The function visualizes the DFA with:</p> <ul> <li>states as circles (double circles for final states)</li> <li>directed edges showing transitions between states</li> <li>edge labels containing tables of token IDs and their corresponding text</li> <li>optional red highlighting for edges in the provided trajectory</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from typing import Literal\n&gt;&gt;&gt; from pydantic import BaseModel, Field\n&gt;&gt;&gt; from transformers import AutoTokenizer\n&gt;&gt;&gt; from litelines import build_dfa\n&gt;&gt;&gt;\n&gt;&gt;&gt; model_id = \"Qwen/Qwen3-0.6B\"\n&gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(model_id)\n&gt;&gt;&gt; draw_dfa(\"A|B\", tokenizer, render=False)\n#\n&gt;&gt;&gt; draw_dfa(\"A0|B0\", tokenizer, render=False)\n#\n&gt;&gt;&gt;\n&gt;&gt;&gt; class Sentiment(BaseModel):\n...     \"Correctly inferred `Sentiment` with all the required parameters with correct types.\"\n...\n...     label: Literal[\"positive\", \"negative\"] = Field(\n...         ..., description=\"Sentiment of the text\"\n...     )\n&gt;&gt;&gt; draw_dfa(Sentiment, tokenizer, whitespace_pattern=\"\")\n#\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>dfa</code> <code>Union[dict[int, dict[int, int]], str, Type[Any]]</code> <p>The DFA representation, which can be either: A dictionary mapping states to their transitions A JSON schema string A Pydantic schema</p> required <code>tokenizer</code> <code>Union[PreTrainedTokenizer, PreTrainedTokenizerFast]</code> <p>The tokenizer used to decode token IDs into readable text</p> required <code>trajectory</code> <code>list</code> <p>Optional list of tokens representing a path through the DFA</p> <code>[]</code> <code>include_tool_call</code> <code>optional</code> <p>Is the Pydantic model expecting a tool call or not.</p> <code>False</code> <code>tool_call_start</code> <code>optional</code> <p>The expected tool call start.</p> <code>'&lt;tool_call&gt;'</code> <code>tool_call_end</code> <code>optional</code> <p>The expected tool call end.</p> <code>'&lt;/tool_call&gt;'</code> <code>whitespace_pattern</code> <code>optional</code> <p>Pattern to use for JSON syntactic whitespace.</p> <code>'[\\\\n\\\\t ]*'</code> <code>max_labels_per_edge</code> <code>optional</code> <p>Maximum number of labels to show per edge</p> <code>3</code> <code>remove_outer_whitespace</code> <code>optional</code> <p>Whether to strip whitespace from token labels in the table.</p> <code>True</code> <code>render</code> <code>optional</code> <p>Whether to return a rendered Graphviz Source object or raw DOT string</p> <code>True</code> <p>Returns:</p> Type Description <code>str | None</code> <p>A Graphviz Source object if render=True, otherwise the DOT language string</p> Source code in <code>src/litelines/draw_dfa.py</code> <pre><code>def draw_dfa(\n    dfa: Union[dict[int, dict[int, int]], str, Type[Any]],\n    tokenizer: Union[PreTrainedTokenizer, PreTrainedTokenizerFast],\n    trajectory: list = [],\n    include_tool_call: bool = False,\n    tool_call_start: str = \"&lt;tool_call&gt;\",\n    tool_call_end: str = \"&lt;/tool_call&gt;\",\n    whitespace_pattern: str = r\"[\\n\\t ]*\",\n    max_labels_per_edge: int = 3,\n    remove_outer_whitespace: bool = True,\n    ratio: Optional[Union[float, str]] = None,\n    size: Optional[Union[Tuple[float, float], str]] = None,\n    render: bool = True,\n) -&gt; str | None:\n    \"\"\"Create a graphical representation of a Deterministic Finite Automaton (DFA) using Graphviz DOT language.\n\n    The function visualizes the DFA with:\n\n    - states as circles (double circles for final states)\n    - directed edges showing transitions between states\n    - edge labels containing tables of token IDs and their corresponding text\n    - optional red highlighting for edges in the provided trajectory\n\n    Examples:\n        &gt;&gt;&gt; from typing import Literal\n        &gt;&gt;&gt; from pydantic import BaseModel, Field\n        &gt;&gt;&gt; from transformers import AutoTokenizer\n        &gt;&gt;&gt; from litelines import build_dfa\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; model_id = \"Qwen/Qwen3-0.6B\"\n        &gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(model_id)\n        &gt;&gt;&gt; draw_dfa(\"A|B\", tokenizer, render=False)\n        #\n        &gt;&gt;&gt; draw_dfa(\"A0|B0\", tokenizer, render=False)\n        #\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; class Sentiment(BaseModel):\n        ...     \"Correctly inferred `Sentiment` with all the required parameters with correct types.\"\n        ...\n        ...     label: Literal[\"positive\", \"negative\"] = Field(\n        ...         ..., description=\"Sentiment of the text\"\n        ...     )\n        &gt;&gt;&gt; draw_dfa(Sentiment, tokenizer, whitespace_pattern=\"\")\n        #\n\n    Args:\n        dfa: The DFA representation, which can be either:\n            A dictionary mapping states to their transitions\n            A JSON schema string\n            A Pydantic schema\n        tokenizer: The tokenizer used to decode token IDs into readable text\n        trajectory: Optional list of tokens representing a path through the DFA\n        include_tool_call (optional): Is the Pydantic model expecting a tool call or not.\n        tool_call_start (optional): The expected tool call start.\n        tool_call_end (optional): The expected tool call end.\n        whitespace_pattern (optional): Pattern to use for JSON syntactic whitespace.\n        max_labels_per_edge (optional): Maximum number of labels to show per edge\n        remove_outer_whitespace (optional): Whether to strip whitespace from token labels in the table.\n        render (optional): Whether to return a rendered Graphviz Source object or raw DOT string\n\n    Returns:\n        A Graphviz Source object if render=True, otherwise the DOT language string\n    \"\"\"\n\n    if isinstance(dfa, dict) and all(\n        isinstance(k, int)\n        and isinstance(v, dict)\n        and all(isinstance(k2, int) and isinstance(v2, int) for k2, v2 in v.items())\n        for k, v in dfa.items()\n    ):\n        regex = \"\"\n        dfa = dfa\n    elif isinstance(dfa, str):\n        if is_valid_json(dfa):\n            regex = build_regex(\n                dfa,\n                include_tool_call=include_tool_call,\n                whitespace_pattern=whitespace_pattern,\n            )\n            dfa = build_dfa(\n                dfa,\n                tokenizer=tokenizer,\n                include_tool_call=include_tool_call,\n                whitespace_pattern=whitespace_pattern,\n            )\n        elif is_valid_regex(dfa):\n            regex = dfa\n            dfa = build_dfa(\n                dfa,\n                tokenizer=tokenizer,\n                include_tool_call=include_tool_call,\n                whitespace_pattern=whitespace_pattern,\n            )\n        else:\n            invalid_schema_error(dfa)\n    elif hasattr(dfa, \"model_json_schema\"):\n        regex = build_regex(\n            dfa,\n            include_tool_call=include_tool_call,\n            whitespace_pattern=whitespace_pattern,\n        )\n        dfa = build_dfa(\n            dfa,\n            tokenizer=tokenizer,\n            include_tool_call=include_tool_call,\n            whitespace_pattern=whitespace_pattern,\n        )\n    else:\n        invalid_schema_error(dfa)\n\n    if trajectory != []:\n        state_trajectory = from_token_trajectory_to_state_trajectory(trajectory, dfa)\n\n    states = range(len(dfa) + 1)\n    final_states = {state for state in states if state not in list(dfa.keys())}\n    graph_str = \"// Allowed Transitions Graph\\ndigraph {\"\n    if regex != \"\":\n        graph_str += f'\\n\\tgraph [label=\"Allowed Paths\\nRegular expression: {build_escaped_title(regex)}\",labelloc=\"t\",labeljust=\"l\"]'\n    else:\n        graph_str += '\\n\\tgraph [label=\"Allowed Paths\",labelloc=\"t\",labeljust=\"l\"]'\n    graph_str += f'\\n\\trankdir=LR;size=\"{size}\";ratio={ratio};'\n    # Add states to the graph\n    for state in states:\n        if state in final_states:\n            # Shape the final states with double circle\n            graph_str += f'\\n\\t{state} [label=\"{state}\" shape=doublecircle]'\n        else:\n            # Shape the other states with a circle\n            graph_str += f'\\n\\t{state} [label=\"{state}\" shape=circle]'\n    # Add empty fake node for initial arrow\n    graph_str += '\\n\\tnode [shape=none]\\n\\t\"\" [label=\"\"]\\n\\t\"\" -&gt; 0'\n    # Put together all edges from state to next_state to the graph\n    all_edges = defaultdict(list)\n    for state, transitions in dfa.items():\n        for key, next_state in transitions.items():\n            all_edges[(state, next_state)].append(key)\n    # Add edges to the graph\n    for state in states:\n        for next_state in states:\n            if all_edges[(state, next_state)] != []:\n                table_str = create_table(\n                    all_edges[(state, next_state)],\n                    tokenizer,\n                    max_labels_per_edge=3,\n                    remove_outer_whitespace=True,\n                )\n                if (\n                    trajectory != []\n                    and state_trajectory != {}\n                    and state in state_trajectory.keys()\n                    and next_state in state_trajectory[state]\n                ):\n                    graph_str += f\"\\n\\t{state} -&gt; {next_state} [label=&lt;{table_str}&gt; color=red penwidth=3.0]\"\n                else:\n                    graph_str += f\"\\n\\t{state} -&gt; {next_state} [label=&lt;{table_str}&gt;]\"\n    graph_str += \"\\n}\\n\"\n    return display_dot_graph(dot=graph_str, render=render)\n</code></pre>"},{"location":"reference/#schema_processor","title":"<code>Schema_Processor</code>","text":"<p>               Bases: <code>LogitsProcessor</code></p> <p>Build the Logits Processor that enforces the response format</p> <p>Examples:</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <p>Token IDs of shape (batch_size, sequence_length)</p> required <code>scores</code> <p>Logits of shape (batch_size, vocab_size)</p> required <p>Returns:</p> Type Description <p>The logits processor that enforces the response format</p> Source code in <code>src/litelines/transformers/schemaprocessor.py</code> <pre><code>class SchemaProcessor(LogitsProcessor):\n    \"\"\"Build the Logits Processor that enforces the response format\n\n    Examples:\n\n    Args:\n        input_ids: Token IDs of shape (batch_size, sequence_length)\n        scores: Logits of shape (batch_size, vocab_size)\n\n    Returns:\n        The logits processor that enforces the response format\n    \"\"\"\n\n    def __init__(\n        self,\n        response_format: Union[str, dict[int, dict[int, int]], Type[Any]],\n        tokenizer: PreTrainedTokenizer,\n        include_tool_call: bool = False,\n        tool_call_start: str = \"&lt;tool_call&gt;\",\n        tool_call_end: str = \"&lt;/tool_call&gt;\",\n        allow_preamble: bool = False,\n        whitespace_pattern: str = r\"[\\n\\t\\r ]*\",\n        verbose: bool = False,\n        max_same_state_visit_count: int = 5,\n    ) -&gt; None:\n        self.response_format = response_format\n        self.tokenizer = tokenizer\n        self.include_tool_call = include_tool_call\n        self.tool_call_start = tool_call_start\n        self.tool_call_end = tool_call_end\n        self.allow_preamble = allow_preamble\n        self.whitespace_pattern = whitespace_pattern\n        self.dfa = None\n        self.verbose = verbose\n        self.max_same_state_visit_count = max_same_state_visit_count\n        self.same_state_visit_count = 0\n        self.current_state = None\n        self.previous_state = None\n        self.final_states = None\n        self.selected_token = None\n        self.trajectory = []\n        self.previous_input_ids = None\n        self.trigger_token_ids = []\n        self.triggered = None\n\n    def __build_dfa(self):\n        self.dfa = build_dfa(\n            self.response_format,\n            self.tokenizer,\n            include_tool_call=self.include_tool_call,\n            tool_call_start=self.tool_call_start,\n            tool_call_end=self.tool_call_end,\n            whitespace_pattern=self.whitespace_pattern,\n        )\n\n    def __create_dfa(self):\n        if isinstance(self.response_format, dict) and all(\n            isinstance(k, int)\n            and isinstance(v, dict)\n            and all(isinstance(k2, int) and isinstance(v2, int) for k2, v2 in v.items())\n            for k, v in (self.response_format).items()\n        ):\n            self.dfa = self.response_format\n        elif isinstance(self.response_format, str):\n            self.__build_dfa()\n        elif hasattr(self.response_format, \"model_json_schema\"):\n            self.__build_dfa()\n        else:\n            raise ValueError(\n                f\"Cannot parse schema {self.response_format}. The schema must be either \"\n                + \"a Pydantic model, a dict[int, dict[int, int]] or a string that contains the JSON \"\n                + \"schema specification\"\n            )\n\n    def show_graph(self):\n        if self.trajectory == []:  # first time\n            self.__create_dfa()\n        return draw_dfa(\n            self.response_format,\n            self.tokenizer,\n            self.trajectory,\n            self.include_tool_call,\n            self.tool_call_start,\n            self.tool_call_end,\n            self.whitespace_pattern,\n        )\n\n    def reset_state(self):\n        \"\"\"Reset the processor to its initial state\"\"\"\n        self.current_state = 0\n        self.final_states = None\n        self.selected_token = None\n        self.trajectory = []\n\n    def __call__(\n        self, input_ids: torch.LongTensor, scores: torch.FloatTensor\n    ) -&gt; torch.FloatTensor:\n        scores_processed = scores.clone()\n        token_chosen_id = torch.argmax(scores_processed).item()\n\n        if self.previous_input_ids is not None:\n            # Check if we're continuing from the previous sequence\n            if not torch.equal(input_ids[:, :-1], self.previous_input_ids):\n                # If the history doesn't match, reset the state\n                self.reset_state()\n\n        if self.final_states is None:  # first time\n            if self.dfa is None:\n                self.__create_dfa()\n            states = range(len(self.dfa) + 1)\n            self.final_states = {\n                state for state in states if state not in list((self.dfa).keys())\n            }\n            if self.verbose:\n                print(f\"states: {states}\")\n                print(f\"final states: {self.final_states}\")\n            self.previous_input_ids = input_ids.clone()\n            if not self.allow_preamble:\n                self.current_state = 0  # dfa active\n                self.triggered = True\n            else:\n                self.current_state = -1  # inactive\n                self.triggered = False\n                # add eos to triggers\n                self.trigger_token_ids = [\n                    self.tokenizer.eos_token_id,\n                    self.tokenizer.pad_token_id,\n                ]\n                if self.include_tool_call:  # it should be a tool call\n                    if (\n                        len(\n                            self.tokenizer.encode(\n                                self.tool_call_start, add_special_tokens=False\n                            )\n                        )\n                        &gt; 1\n                    ):\n                        raise ValueError(\n                            f\"{self.tool_call_start} is not a valid token.\"\n                        )\n                    self.trigger_token_ids.append(\n                        self.tokenizer.encode(\n                            self.tool_call_start, add_special_tokens=False\n                        )[0]\n                    )\n                    # not the best solution since it excludes '&lt;' in the preamble\n                    tokens_containing_open_tool_call = [\n                        token_id\n                        for token_id in range(self.tokenizer.vocab_size)\n                        if \"&lt;\" in self.tokenizer.decode(token_id)\n                    ]\n                    self.trigger_token_ids += tokens_containing_open_tool_call\n                else:  # it should be json\n                    tokens_containing_open_curly_bracket = [\n                        token_id\n                        for token_id in range(self.tokenizer.vocab_size)\n                        if \"{\" in self.tokenizer.decode(token_id)\n                    ]\n                    self.trigger_token_ids += tokens_containing_open_curly_bracket\n\n        else:  # not the first time\n            self.selected_token = input_ids[:, -1].item()\n            if self.current_state != -1:\n                self.trajectory.append(self.selected_token)\n            if self.verbose:\n                print(\n                    f\"\\x1b[32mselected token: {self.selected_token}: {repr(self.tokenizer.decode([self.selected_token]))}\\x1b[0m\"\n                )\n            if self.verbose and self.current_state != -1:\n                print(f\"mapping: {self.dfa[self.current_state]}\")\n\n        # activate it if it is triggered\n        if (\n            self.current_state == -1 and token_chosen_id in self.trigger_token_ids\n        ):  # if dfa is inactive\n            if self.verbose:\n                print(\n                    f\"\\x1b[31mtrigger token: {token_chosen_id}: {self.tokenizer.decode([token_chosen_id])}\\x1b[0m\"\n                )\n            self.triggered = True\n            self.current_state = 0\n\n        if self.current_state != -1:  # if dfa is active\n            if self.triggered:\n                self.current_state = 0\n                self.triggered = False\n            else:\n                self.previous_state = self.current_state\n                self.current_state = self.dfa[self.current_state][self.selected_token]\n                if (\n                    self.previous_state == self.current_state\n                    and re.fullmatch(\n                        self.whitespace_pattern,\n                        self.tokenizer.decode([self.selected_token]),\n                    )\n                    is not None\n                ):\n                    self.same_state_visit_count += 1\n                else:\n                    self.same_state_visit_count = 0\n\n        self.previous_input_ids = input_ids.clone()\n\n        vocab_tensor = torch.arange(scores.shape[-1], device=scores.device)\n\n        if self.verbose:\n            print(f\"\\x1b[34mcurrent state: {self.current_state}\\x1b[0m\")\n            print(\n                f\"\\x1b[33msame state visit count: {self.same_state_visit_count}\\x1b[0m\"\n            )\n\n        if self.current_state == -1:\n            # print(self.trigger_token_ids)\n            forbidden_tokens = torch.tensor(\n                self.trigger_token_ids, device=scores.device\n            )\n            forbidden_tokens_mask = torch.isin(vocab_tensor, forbidden_tokens)\n        else:  # if dfa is active\n            if self.current_state in self.final_states:\n                allowed_tokens = [self.tokenizer.eos_token_id]\n            else:\n                if self.same_state_visit_count &lt; self.max_same_state_visit_count:\n                    allowed_tokens = list(self.dfa[self.current_state].keys())\n                else:\n                    # Remove tokens that send you to the same current state\n                    if self.verbose:\n                        print(\n                            f\"\\x1b[31mmaximum same state visit count reached for state {self.current_state}\\x1b[0m\"\n                        )\n                    mapping = self.dfa[self.current_state]\n                    allowed_tokens = [\n                        key\n                        for key, value in mapping.items()\n                        if value != self.current_state\n                    ]\n            allowed_tokens = torch.tensor(allowed_tokens, device=scores.device)\n            forbidden_tokens_mask = ~torch.isin(vocab_tensor, allowed_tokens)\n\n        scores_processed = torch.where(forbidden_tokens_mask, -torch.inf, scores)\n        if self.verbose:\n            print(\n                f\"\\x1b[35mwill be chosen: {torch.argmax(scores_processed).item()}\\x1b[0m\"\n            )\n\n        return scores_processed\n</code></pre>"},{"location":"reference/#src.litelines.transformers.schemaprocessor.SchemaProcessor.reset_state","title":"<code>reset_state()</code>","text":"<p>Reset the processor to its initial state</p> Source code in <code>src/litelines/transformers/schemaprocessor.py</code> <pre><code>def reset_state(self):\n    \"\"\"Reset the processor to its initial state\"\"\"\n    self.current_state = 0\n    self.final_states = None\n    self.selected_token = None\n    self.trajectory = []\n</code></pre>"}]}